{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a341fa4-b4dc-4cea-a4b3-249aa5fc9394",
   "metadata": {},
   "source": [
    "### Demonstrate the LLM GPT2 Model OnBoarding on Cloud AI 100 Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eef7ea-3488-414c-9e36-e960abba30c9",
   "metadata": {},
   "source": [
    "##### Download the OpenSource GPT2 based HuggingFace Model and Save in local *Cache* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21f82d5-17df-4fc9-a180-05edd032f02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr2/ochougul/.pyenv/versions/3.8.19/envs/py38/lib/python3.8/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2 from hugging-face \n",
      " GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initiate the Orignal Transformer model\n",
    "import os\n",
    "\n",
    "from QEfficient import QEFFAutoModelForCausalLM\n",
    "\n",
    "# Please uncomment and use appropriate Cache Directory for transformers, in case you don't want to use default ~/.cache dir.\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/local/mnt/workspace/hf_cache\"\n",
    "\n",
    "# ROOT_DIR = os.path.dirname(os.path.abspath(\"\"))\n",
    "# CACHE_DIR = os.path.join(ROOT_DIR, \"tmp\") #, you can use a different location for just one model by passing this param as cache_dir in below API.\n",
    "\n",
    "# Model-Card name to be onboarded (This is HF Model Card name) : https://huggingface.co/gpt2-xl\n",
    "model_name = \"gpt2\"  # Similar, we can change model name and generate corresponding models, if we have added the support in the lib.\n",
    "\n",
    "qeff_model = QEFFAutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"{model_name} from hugging-face \\n\", qeff_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dfa0a-d8fe-4472-bf00-55e563ae9058",
   "metadata": {},
   "source": [
    "##### Now we Modify the GPT2 Classes using the Optimized Software Library to generate model for Cloud AI 100.\n",
    "##### Here we generate models with below Optimizations:\n",
    "\n",
    "* RMS Norm Fixes for FP16 Overflows and Underflow\n",
    "* Causal Mask Fix\n",
    "* Handling FP16 Overflows.\n",
    "* KV Cache (Retention Changes).\n",
    "* Triu/Tril Ops support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4543b94-9b50-4bcc-90c6-484ab694c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20mWARNING - QEfficient - The model <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> layers has been upadted to QEff layers in-place\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model after Optimized transformations \n",
      " QEffGPT2LMHeadModel(\n",
      "  (transformer): QEffGPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x QEffGPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): QEffGPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import QEfficient\n",
    "\n",
    "# Easy and minimal api to update the model\n",
    "model_transformed = QEfficient.transform(qeff_model, form_factor=\"cloud\")\n",
    "\n",
    "print(\"Model after Optimized transformations \\n\", model_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1795ac7-d92c-42bb-8148-cb9da87439a6",
   "metadata": {},
   "source": [
    "##### Export the Optimized Pytorch model to the Onnx Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb4d6dd-9973-4608-b68b-ec6825cfef0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7fe36d84a24006ba52887588e9935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33;20mWARNING - QEfficient - Overriding /local/mnt/workspace/open-source/myown/efficient-transformers/qeff_models/gpt2/onnx\u001b[0m\n",
      "/local/mnt/workspace/open-source/myown/efficient-transformers/QEfficient/transformers/models/gpt2/modeling_gpt2.py:247: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n",
      "/local/mnt/workspace/open-source/myown/efficient-transformers/QEfficient/transformers/models/gpt2/modeling_gpt2.py:498: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert value.shape[2] == seq_length\n",
      "/local/mnt/workspace/open-source/myown/efficient-transformers/QEfficient/transformers/models/gpt2/modeling_gpt2.py:402: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attention_mask_RetainedState=attention_mask_retained if past_length > 0 else None,\n",
      "[W export.cpp:565] Warning: Custom opset domain: 'com.qti.aisw.onnx' provided is not used in the model. Please verify custom opset domain names. (function GraphEncoder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.0+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\n",
      "=============== PyTorch vs. fp32 ONNXRT (MAD) ===============\n",
      "\n",
      "logits \t\t 7.62939453125e-05\n",
      "attention_mask_RetainedState \t\t 0.0\n",
      "past_keys (mean) \t\t 2.635022004445394e-06\n",
      "past_value (mean) \t\t 5.5730342864990234e-06\n",
      "\n",
      "=============================================================\n",
      "\n",
      "\n",
      "=============== PyTorch vs. fp16 ONNXRT (MAD) ===============\n",
      "\n",
      "logits \t\t 7.62939453125e-05\n",
      "attention_mask_RetainedState \t\t 0.0\n",
      "past_keys (mean) \t\t 2.635022004445394e-06\n",
      "past_value (mean) \t\t 5.5730342864990234e-06\n",
      "\n",
      "=============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from QEfficient.utils import load_hf_tokenizer\n",
    "# We can now export the modified models to Onnx framework\n",
    "# This will generate single Onnx Model for both Prefill and Decode Variations which are optimized for\n",
    "# Cloud AI 100 Platform.\n",
    "\n",
    "# This will generate Onnx model, clip the overflow constants to fp16\n",
    "# Verify the model on Onnxruntime vs Pytorch\n",
    "# Then generate inputs and customio yaml file required for compilation.\n",
    "\n",
    "# We can generate the KV Style models with the flag \"kv\"\n",
    "# Bertstyle models do not have any optimization w.r.t KV cache changes and are unoptimized version.\n",
    "# It is recommended to use kv=True for better performance.\n",
    "tokenizer = load_hf_tokenizer(model_name, use_cache=True)\n",
    "base_path, onnx_path = QEfficient.export(\n",
    "    model_name=model_name,\n",
    "    model_kv=model_transformed,\n",
    "    tokenizer=tokenizer,\n",
    "    kv=True,\n",
    "    form_factor=\"cloud\",\n",
    "    return_path=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126872b-8bc0-412e-956f-50eb01e5b6de",
   "metadata": {},
   "source": [
    "##### Compile the Optimized KV Cache Single Model on Cloud AI 100 (**Config; 16C;32PL;128CTX;FP16**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48be5da-02a1-4d7e-9b5f-a6dcca141d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AI 100 compiler: /opt/qti-aic/exec/qaic-exec -m=/local/mnt/workspace/open-source/myown/efficient-transformers/qeff_models/gpt2/onnx/gpt2_kv_clipped_fp16.onnx -aic-hw -aic-hw-version=2.0 -network-specialization-config=/local/mnt/workspace/open-source/myown/efficient-transformers/qeff_models/gpt2/specializations.json -convert-to-fp16 -retained-state -aic-num-cores=14 -custom-IO-list-file=/local/mnt/workspace/open-source/myown/efficient-transformers/qeff_models/gpt2/onnx/custom_io_fp16.yaml -compile-only -aic-binary-dir=/local/mnt/workspace/open-source/myown/efficient-transformers/qeff_models/gpt2/qpcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== Compilation Done! =====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please use platform SDk to Check num_cores for your card.\n",
    "\n",
    "generated_qpc_path = QEfficient.compile(\n",
    "    onnx_path=onnx_path,\n",
    "    num_cores=14,\n",
    "    qpc_path=os.path.dirname(base_path),\n",
    "    mxfp6=False,\n",
    "    device_group=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92aa240c-f40b-4bf8-a982-8ffff4ff3978",
   "metadata": {},
   "source": [
    "##### Execute the Optimized KV Model on H/W and Print the Latency Stats *(tok/sec)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4711fc74-aa5d-4e20-af0e-0d461d2e19bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 My name is  John .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man  of  God .  I 'm  a  man \n",
      "\n",
      "===================== Performance Stats =====================\n",
      "Prefill time a.k.a TTFT is= 0.01 s\n",
      "Decode: 220.31 tok/s\n",
      "E2E: 216.88 tok/s\n",
      "Total (E2E) inference time is= 0.44 s\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "from QEfficient.generation.text_generation_inference import get_compilation_batch_size\n",
    "\n",
    "# post compilation, we can print the latency stats for the kv models, We provide API to print token and Latency stats on AI 100\n",
    "# We need the compiled prefill and decode qpc to compute the token generated, This is based on Greedy Sampling Approach\n",
    "batch_size = get_compilation_batch_size(generated_qpc_path)\n",
    "QEfficient.cloud_ai_100_exec_kv(batch_size=batch_size, tokenizer=tokenizer, qpc_path=generated_qpc_path, device_id=[0], prompt=[\"My name is\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
